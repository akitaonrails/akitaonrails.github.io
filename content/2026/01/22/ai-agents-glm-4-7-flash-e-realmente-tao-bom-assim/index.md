---
title: "AI Agents: GLM 4.7 Flash √© realmente t√£o bom assim?"
slug: "ai-agents-glm-4-7-flash-e-realmente-tao-bom-assim"
date: 2026-01-22T13:41:05-0300
tags:
- crush
- zai
- glm
---

Recentemente fiz [um post comparando v√°rias LLMs](https://akitaonrails.com/2026/01/11/ai-agents-comparando-as-principais-llm-no-desafio-de-zig/). A boa not√≠cia √© que as LLMs comerciais est√£o bem impressionantes mesmo. Mas a m√° not√≠cia √© que nenhuma LLM open source foi capaz de realizar meu desafio de programa√ß√£o (leia no post anterior pra entender o desafio).

Tamb√©m recentemente, a ZAI lan√ßou a vers√£o 4.7 do seu famoso modelo ["GLM"](https://huggingface.co/zai-org/GLM-4.7) que √© um modelo de 30 bilh√µes de par√¢metros, encodado em BF16, que precisa na faixa de 60GB de VRAM pra caber inteiro numa GPU. Ou seja, √© pesado.

Ainda mais recentemente, ela lan√ßou a vers√£o [**GLM 4.7 Flash**](https://huggingface.co/zai-org/GLM-4.7-Flash) que √© o mesmo modelo de 30 bilh√µes de par√¢metros, mas quantizado (a grosso modo: "comprimido" ou "truncado e re-normalizado"), que cabe numa GPU de pelo menos 32GB, que √© o caso da minha **RTX 5090**.

Sim, a GPU caseira mais cara do mercado √© o m√≠nimo necess√°rio pra rodar os melhores modelos open source. Isso porque o importante n√£o √© o poder de processamento e sim a quantidade de VRAM. Por isso um Mac Studio, com GPU tecnicamente mais "fraca", tem vantagem em LLMs porque ela compartilha com a RAM, com m√°ximo te√≥rico de 512GB. Ent√£o cabem os maiores modelos sem problema nenhum.

Outras op√ß√µes s√£o Mini PCs com CPU AMD Ryzen AI Max+ com capacidade de compartilhar at√© 128GB de VRAM.

O desafio era ver se eu conseguia rodar o GLM 4.7 Flash localmente na minha m√°quina.

> **TL;DR: GLM 4.7 Flash √© possivelmente o melhor LLM open source - com alguns por√©ns.**

### VLLM vs LM Studio vs Ollama

Na [p√°gina oficial](https://huggingface.co/zai-org/GLM-4.7-Flash) deles tem documenta√ß√£o de como rodar em VLLM e j√° aviso que n√£o funciona. Ele exige funcionalidades que s√≥ tem na branch master dos projetos VLLM e transformers e vai dar uma dor de cabe√ßa com conflitos de vers√µes e resolu√ß√£o de depend√™ncias e mesmo assim, no final, eu n√£o consegui fazer ele rodar direito.

LM Studio tamb√©m n√£o funciona. Ele tem a op√ß√£o pra baixar o modelo, e eu tentei tunar todos os par√¢metros poss√≠veis. Quantiza√ß√£o do KV Cache, layers na GPU, CPU threads, etc etc. Mas n√£o importa, o modelo at√© carrega mas roda absurdamente lento. T√£o lento que √© inus√°vel. Acho que √© porque ele s√≥ consegue baixar o modelo BF16, ent√£o metade dos layers n√£o cabe na GPU. Portanto, at√© a data deste post, n√£o d√° pra usar nem LM Studio e nem VLLM.

Mas a√≠ eu vi um [tweet](https://x.com/ollama/status/2013372316021834086) dizendo que finalmente Ollama ia suportar GLM 4.7, mas novamente, at√© a data deste post, ainda n√£o est√° na vers√£o est√°vel. A √∫nica forma de usar √© compilando do c√≥digo fonte as coisas que est√£o no branch master deles.

```bash
git clone https://github.com/ollama/ollama.git
cd ollama
cmake -B build
cmake --build build
go build -v .
```

No √∫ltimo comando vai parecer que travou porque nada aparece na tela, mas √© porque ele est√° compilando centenas de arquivos de C++. S√≥ esperar que uma hora acaba. E finalmente d√° pra rodar:

```bash
OLLAMA_FLASH_ATTENTION=1 \
GGML_VK_VISIBLE_DEVICES=-1 \
OLLAMA_KV_CACHE_TYPE=q4_0 \
OLLAMA_CONTEXT_LENGTH=45000 \
go run . serve
```

Esses s√£o os par√¢metros pra conseguir caber tudo nos 32GB de VRAM e n√£o ficar fazendo offload pra CPU. 45 mil tokens √© o m√°ximo, talvez at√© um pouco menos pra garantir que o KV Cache vai caber.

Al√©m disso precisa quantizar em 4-bits com `q4_0`. O normal s√£o floats 16-bits. Pra caber vamos ter que truncar (perder precis√£o).

No meu caso, eu tenho uma iGPU AMD. Pra evitar que o ollama fique tentando usar ela, tem essa op√ß√£o `GGML_VK_VISIBLE_DEVICES=-1`. Isso deve garantir que s√≥ a nvidia vai ser usada.

Finalmente, habilitamos **Flash Attention** pra tentar reduzir crescimento de mem√≥ria √† medida que janela de contexto aumenta.

### Trade off: velocidade vs contexto

O GLM 4.7 Flash √© um modelo com 48 layers. Na configura√ß√£o que mostrei acima, vai caber todos os 48 layers na GPU. Mas uma janela de contexto de 45k **pode** ser muito pequeno dependendo do seu uso. Voc√™ mesmo vai ter que testar.

![log gpu](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/20260123101949_screenshot-2026-01-23_10-09-35.png)

Quando cabe tudo na GPU, o processamento √© razoavelmente r√°pido. Um pouco mais lento mas ainda compar√°vel a um Qwen3 Coder ou GPT-OSS. D√° pra saber que est√° tudo na GPU porque olhando BTOP a CPU est√° sem processar nada e ao mesmo tempo `nvidia-smi` mostra que a GPU t√° puxando na faixa de 500W dos 600W que ele consegue.

![nvidia-smi 500w](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/20260123102147_screenshot-2026-01-22_18-50-42.png)

Nos meus testes, dependendo do que estiver fazendo, esses 45k enchem **MUITO R√ÅPIDO**. Usando ferramentas como OpenCode ou Crush, quando chega perto de encher, eles pedem pra LLM sumarizar o que foi conversado/pesquisado at√© agora, gravar e reiniciar com o resumo. Mas mesmo assim, os resumos tamb√©m v√£o ficando grandes.

Em agentes, ele vai executar comandos no seu sistema (compilar, listar arquivos, pesquisar trechos de c√≥digo, pesquisar na web) e tudo isso vai consumir contexto. Quanto mais contexto tiver sobrando, melhor.

Eu andei testando com `OLLAMA_CONTEXT_LENGTH=65576`, faixa dos 65k tokens. Isso vai impedir todos os layers de ficarem na GPU e o Ollama vai ter que ficar fazendo swap, usando CPU. Portanto, o processamento vai ficar bem mais lento. MAS, a janela de contexto vai ser maior. √â um trade-off.

![btop](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/20260123102557_screenshot-2026-01-22_20-51-05.png)

De cara, voc√™ vai ber que a CPU vai come√ßar a puxar. Na minha 7850x3d fica puxando 50% constantemente. Veja os tempos das respostas do Ollama como ficou lento:

![ollama cpu](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/20260123102645_screenshot-2026-01-23_10-09-50.png)

Vai da faixa de 4 segundos pra 90 segundos. 20x mais lento. E isso porque 39 dos 48 layers est√£o na GPU. Por isso eu acho que n√£o d√° pra aumentar mais que 65k. O ideal √© manter em 45k mesmo, mas no caso do meu desafio, senti que precisava compensar.

Ou o Crush/OpenCode vai demorar porque a janela acaba r√°pido demais e toda hora precisa sumarizar o trabalho e perder detalhes do contexto - o que d√° mais trabalho em pesquisar a mesma coisa mais de uma vez. Ou preserva mais detalhes do contexto por mais tempo, mas a√≠ todas as respostas ficam mais lentas. √â um trade-off bem dif√≠cil. E a culpa de tudo √© a porcaria dessas placas de video terem t√£o pouca VRAM.

Se usar um Mac Studio que vai at√© 512GB ou algum Mini-PC com AMD Ryzen AI Max+ que vai at√© 128GB, n√£o ter√≠amos que lidar com esse limite. Mas s√£o m√°quinas que custam acima de BRL 20 mil. N√£o d√° pra justificar. Por esse pre√ßo √© melhor s√≥ pagar cr√©ditos na OpenRouter e usar modelos comerciais maiores e mais r√°pidos como Claude Opus ou GPT 5.2, ou o pr√≥prio GLM 4.7 no cloud da ZAI.

### Pull Request no Crush

Como side-quest, tive um pequeno problema que tava me incomodando. Um dos problemas de uma janela de contexto pequena, √© que de tempos em tempos o Crush ficava parando, mesmo ainda tendo trabalho pra fazer. O motivo √© porque o contexto encheu antes dele ter tempo de sumarizar da√≠ quando vai fazer uma chamada de ferramenta ("tool calling"), que √© um XML grande, ele trunca por falta de espa√ßo.

XML truncado √© inv√°lido, ele n√£o consegue chamar ferramenta e crasheia o processamento, assim:

![log xml](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/20260123103238_screenshot-2026-01-22_15-12-36.png)

E mesmo tentando controlar o limite de uso da mem√≥ria, eventualmente esse teto vai quebrar e o processamento vai parar por falta de mem√≥ria:

![log nvm](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/20260123103736_screenshot-2026-01-23_10-36-43.png)

Seja por XML truncado, seja por OOM do ollama, eventualmente o processamento p√°ra. E como √© um processamento lento, √© um saco ter que ficar esperando isso acontecer e reiniciar o processo manualmente.

Por isso resolvi ver se tinha como fazer o pr√≥prio Crush detectar que parou por causa desses motivos e auto-continuar de onde parou. Abri o pr√≥prio Crush em cima do projeto do Crush e pedi pro Claude Opus checar a possibilidade de uma corre√ß√£o.

[![pull request](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/20260123105702_screenshot-2026-01-23_10-56-40.png)](https://github.com/charmbracelet/crush/pull/1962)

Sim, eu fiz um pull request 100% vibe coded com o pr√≥prio Crush pra consertar o Crush üòÇ Vamos ver se o povo do Crush vai aceitar. Eu tentei o m√°ximo n√£o fazer um [AI Slop](https://www.youtube.com/watch?v=vrTrOCQZoQE&pp=ygUHYWkgc2xvcNIHCQmHCgGHKiGM7w%3D%3D).

De qualquer forma, com isso eu consigo deixar o agente rodando horas sem parar at√© chegar onde eu quero.

### Conclus√£o

> Este N√ÉO √© o jeito certo!

Deixando claro que largar um agente fazendo coisas sozinho sem ningu√©m ver √© uma m√°-pr√°tica. No meu caso √© um exerc√≠cio isolado. Eu estou usando meu [AI JAIL](https://akitaonrails.com/2026/01/10/ai-agents-garantindo-a-protecao-do-seu-sistema/) e eu j√° rodei esse prompt v√°rias vezes como benchmark e sei bem o comportamento de todas as LLMs.

Como disse no meu post anterior, o prompt do meu desafio √© cheio de m√°s pr√°ticas que voc√™ n√£o deve fazer: eu pe√ßo muitas coisas complexas ao mesmo tempo, com pouco contexto, de forma vaga e confusa, justamente pra ver como a LLM vai se virar pra resolver. No dia a dia voc√™ deve mandar prompts pequenos, tarefas unit√°rias objetivas e bem espef√≠cas, com boa explica√ß√£o e contexto, pra que as respostas sejam r√°pidas e curtas. Um passo de cada vez.

O objetivo do meu desafio √© justamente puxar um pouco os limites do senso comum. Com a id√©ia de que se passar pelo meu desafio, deve conseguir passar por prompts mais simples do dia a dia.

E nesse desafio que s√≥ as LLMs comerciais conseguiram passar: Claude Opus, GPT 5.2, Gemini 3. Mas nenhum open source conseguiu, nem Qwen3 Coder, nem GPT-OSS, nem minha primeira tentativa com GLM 4.7 cloud e nem MiniMax v2.

Mas este novo GLM 4.7 Flash me deu esperan√ßas. Ele n√£o entrou em "agentic loop" como o Qwen3 - que sempre chega num ponto onde ele p√°ra de tentar resolver e s√≥ repete "j√° terminei", sem ter terminado.

Tamb√©m n√£o ficou crasheando a toa. Toda vez que parou o processamento, foi por falta de mem√≥ria. E mesmo assim, mandando "continuar" via prompt no Crush, ele consegue pegar de onde parou e continuar.

Cada corre√ß√£o que ele faz tem sentido. N√£o vi fazendo tangentes fora do escopo como vi o Gemini fazendo muitas vezes. Quando tem d√∫vidas, consegue pedir um "agentic fetch" e ir pesquisar na web antes de continuar. Toda vez que faz corre√ß√µes ele sabe executar o comando de build pra ver se tem erros de compila√ß√£o.

Gostei que tenta corrigir um pequeno erro de cada vez em vez de tentar ficar reescrevendo tudo ou resolvendo mais de um problema ao mesmo tempo.

> No final, foi o √∫nico open source que fez o build funcionar e gerar um bin√°rio.

N√£o dava pra fazer ele testar o programa porque ele precisaria carregar outra LLM numa GPU que j√° estava com LLM, ent√£o eu parei o teste sem resolver bugs de runtime. E tamb√©m n√£o pedi pra ele refatorar porque j√° tinha levado tempo demais. Ele j√° tinha superado os outros OSS ent√£o parei por aqui. Eis o [pull request](https://github.com/akitaonrails/qwen-cli-zig/pull/6) com as corre√ß√µes de build que ele fez. Simples, sem mexer onde n√£o precisa e faz o que foi pedido.

Ou seja, dos open source, foi o que vi o melhor comportamento, compar√°vel ao que vi o Claude Opus ou GPT 5.2 fazendo. Considerando que esses modelos comerciais tem trilh√µes de par√¢metros, rodando nos servidores mais parrudos com NVIDIA H200, eu considero que esse GLM, de s√≥ 30 bilh√µes de par√¢metros, rodando numa m√≠sera 5090 local, est√° se saindo impressionantemente bem.

Os trade-offs s√£o claros: ele √© ordens de grandeza mais lento que um Claude Opus. Mas √© melhor um modelo lento que chega no mesmo resultado, do que um r√°pido que entra em loop infinito achando que j√° resolveu tudo sem ter feito nada, como o Qwen3.

Acho que pra desafios como o meu: prompt gigante, que exige muito contexto pra conseguir resolver, n√£o √© bom pra nenhum modelo open source. Mas se eu fosse come√ßar um projetinho pequeno do zero, com tarefas simples e curtas, imagino que o GLM 4.7 resolveria sem nenhuma dor de cabe√ßa, custando zero e preservando minha privacidade ao rodar 100% local e offline.

Recomendo: GLM 4.7 Flash √© o melhor modelo OSS do come√ßo de 2026.
