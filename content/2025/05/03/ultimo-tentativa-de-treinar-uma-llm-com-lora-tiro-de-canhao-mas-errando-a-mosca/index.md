---
title: √öltimo Tentativa de Treinar uma LLM com LoRa. Tiro de canh√£o, mas errando a
  mosca.
date: '2025-05-03T21:15:00-03:00'
slug: ultimo-tentativa-de-treinar-uma-llm-com-lora-tiro-de-canhao-mas-errando-a-mosca
tags:
- qwen3
- vllm
- runpod
- llm
draft: false
---



Ok, eu sou insistente. no [post anterior](https://www.akitaonrails.com/2025/05/03/ensinando-zig-mais-recente-pra-sua-llm-treinando-loras-quase) expliquei tudo que eu sabia sobre tentar fazer fine-tuning de um modelo com LoRa. Mas √© um processo tedioso, demorado, e limitado √† "pobreza" da minha RTX 4090. Em 24GB de VRAM tem que caber o modelo, coisas como KV cache, dataset de treinamento e espa√ßo pra processar tudo. Apertando, cabe o modelo Qwen3-8B e 1MB de dataset e j√° era. Vai usar 21GB constantemente durante mais de 1 hora e o resultado final, apesar do modelo conseguir responser "sim, eu sei Zig 0.14", n√£o √© bom - porque o modelo base de s√≥ 8B j√° n√£o era grande coisa.

A sa√≠da: migrar meu treinamento pra uma m√°quina maior. Comecei tentando uma A40, pra ver se 40GB era suficiente pra caber o modelo Qwen3-32B pra esse treinamento. E n√£o, n√£o coube.

"Ph0da-se", pensei. Aluguei a maior configura√ß√£o que tem na RunPod:

![H100](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/9m5uitxvhhbpotu5lkbrw0znd276?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2014-08-43.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252014-08-43.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001407Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=8f9ac2076a5ecdab468f06e15c7fa5d5f6fc9e843c1666fdf68d36eec4cfcc6c)

Essa H100 j√° √© "obsoleta", nos datacenters mais modernos j√° existe H200 Blackwell e outras coisas maiores. Mas no mundo mais de "mortais", uma H100 ao custo de USD 3 por HORA, vai ter que servir. O bom: tem 80GB de VRAM. E o que eu n√£o esperava: at√© isso √© pouco!

E aqui a vantagem de ter feito tudo organizado e subir neste [repo no GitHub](https://github.com/akitaonrails/qwen3-zig-lora-training). O setup foi trivial: foi s√≥ escolher subir um pod com image de PyTorch 2.8 com Python 3.10 e Cuda 12.8, que s√£o as vers√µes est√°veis mais recentes. Baixar meu projeto, rodar um `pip install -r requirements.txt` e j√° podia rodar meu `./train-lora.py`.

Eu ia adicionar de volta aqueles 15MB de c√≥digo-fonte da Standard Library do Zig mais novo, mas mesmo na H100, n√£o coube! O treinamento s√≥ com o 1MB original que eu usei ontem tamb√©m, j√° fez a m√°quina ficar entuchada quase no m√°ximo:

![74GB](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/iahqr9hzs2aaku3fup7u3cw9navk?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2018-41-42.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252018-41-42.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001409Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=f66627b08e87bd83977a5f735966d822a32e9e8bcbc6fb2fa73f84e4504ae561)

Olha que absurdo: ficou o tempo todo do treinamento ocupando quase 75GB dos 80GB que tem de VRAM. Literalmente n√£o d√° pra fazer treinamento com muito mais material que isso. Ou precisa descer pra um modelo menor, como o Qwen3-14B. Eu ainda n√£o sei se 32B com menos material ou 14B com mais material, qual seria a melhor combina√ß√£o. Eu j√° sei que 8B n√£o presta pra programar nada complicado. 14B j√° tive um pouco mais de sucesso em testes, mas tamb√©m n√£o faz nada muito complicado. 32B n√£o √© garantia que faz muito melhor: o Qwen2.5-32B n√£o se saiu melhor que o 14B, mas o novo Qwen3-32B tem "cara" que √© mais est√°vel. √â isso que estou querendo testar, com o LoRa.

Isso foi na hora de almo√ßo. Estou escrevendo este post na hora da janta. Esta foi a previs√£o do treinamento:

![6h](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/lzrfdl97n96qesnhde33lugidr3i?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2014-18-12.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252014-18-12.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001410Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=d1b3d8fe1c96ff8451f97b7029cd9e5802836a018efd89c1304c0c7d074c47e9)

Essa estimativa fica variando, mas eu acho que levou em torno de 5 a 6 horas mesmo.

![lora size](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/stoakih27g3o9c1e1d07y14szrf0?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2019-33-08.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252019-33-08.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001411Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=74ff4a3b0c9c41d1764fd6fbb50db6cf59e443e5d32b59f351e23565218aa9cc)

E no final dessas quase 6 horas, foi isso que gerou: um LoRa de 5.1GB (contra os 3GB de ontem). Tamanho n√£o √© documento. N√£o significa que maior √© melhor. Vamos ver.

### Depois do Treino

Agora que acabou o treinamento, posso usar o mesmo pod com a H100 pra tamb√©m servir esse modelo com o Lora, e precisaria ser ele de qualquer forma. Pra rodar √© assim:

```bash
vllm serve Qwen/Qwen3-32B \
    --enable-lora \
    --lora-modules ziglora=./qwen3-zig-lora \
    --max-model-len 10000
```

E precisa limitar ainda o tamanho do contexto. Por default o vLLM tenta subir com contexto de 40k tokens e, mesmo na H100, com 80GB, ele falha a inicializa√ß√£o com este erro:

![KV Cache error](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/ykusgib7o6wybhi5rs4zvx2yrp8j?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2019-41-52.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252019-41-52.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001413Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=d15b568205425d4bf20815f2f106779a4956a39d0c776696c2701516214d515e)

N√£o sobra espa√ßo pra subir o KV Cache (em resumo, no processo de gera√ß√£o de pr√≥xima palavra, vai adicionando uma nova linha e uma nova coluna na matrix, mas n√£o precisa recalcular as linhas/colunas anteriores, √© o mesmo resultado. Da√≠ tem esse cache).

Um comportamento estranho tanto na minha m√°quina quanto na H100. N√£o sei se isso acontece sempre que carrega um Lora, ou se meu Lora tem defeito, ou se tem alguma configura√ß√£o que eu n√£o estou vendo. Mas carregando com Lora, a inicializa√ß√£o √© EXTREMAMENTE LENTA. Subir o vLLM, que normalmente leva 1 minuto, agora leva 5 minutos, ou mais. N√£o sei o que causa isso, mas n√£o √© um tipo de servidor que pode ser facilmente desligado e religado. Faz eu ter saudade de subir um servidor J2EE ... 

Falando s√©rio, parece que n√£o sou s√≥ eu. Esta [Issue](https://github.com/vllm-project/vllm/issues/6876) n√£o teve solu√ß√£o. "Pode" ser algum bug no vLLM, "pode" ser uma combina√ß√£o de fatores que ningu√©m sabe. Mas nem a GPU e nem a CPU est√£o processando nada. CPU n√£o sai de 10% de uso, GPU parece que est√° s√≥ esperando. Realmente, √† primeira vista parece algum problema de I/O. Talvez o processo de checar os shards de checkpoints, puxando de um network volume, seja lento. Mas como vou rodar s√≥ hoje, n√£o me preocupei em debugar isso.

![slow load](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/mluc626od3h1q0swnv8kepe1aji5?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2019-53-59.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252019-53-59.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001414Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=888d340c5f1ce572730ad4f0909299a48bdfd9414161933e363e6d5a579512a7)

Olha isso. 8 MINUTOS nessa parte de carregar checkpoints. E depois ainda continua outras fases que tamb√©m demora mais um pouco. Vou deixar essa foto aqui, caso algu√©m j√° tenha visto e saiba o que pode ser.

S√≥ pra tirar a d√∫vida, experimentei subir o vLLM s√≥ com o Qwen3 e tirando a LoRa completamente e, pra minha surpresa:

![sem lora](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/2hzvcqrk087i5ov4otx0rxa8erdf?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2021-14-05.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252021-14-05.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001416Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=87fdb2b938c19d64ce946a4f940e370c6144e691ee703d748cbfa33f2331ff4f)

Tamb√©m levou quase 8 minutos. Ent√£o o problema n√£o √© a LoRa. Ou √© alguma coisa do vLLM que d√° pra configurar - mas eu n√£o sei como - ou √© uma limita√ß√£o do pr√≥prio modelo Qwen3-32B que √© lento assim mesmo pra carregar. 

Enfim, com Lora, tentei diminuir pra 16k tokens: n√£o sobe. Tentei subir com 10k tokens. E nesse ponto eu diria que esse teste j√° fracassou. Um Qwen3-32 teoricamente tem capacidade de suportar janelas de at√© 40k tokens. N√£o √© grande, mas d√° pra trabalhar um pouco nesse contexto.

Mas abaixo de 15k tokens j√° n√£o d√° pra usar com ferramentas como Aider, que precisa mandar system prompts longos pra instruir a LLM como se comportar pra cuspir c√≥digo que ele consegue capturar. Ent√£o √© isso: um Qwen3-32b + Lora, numa H100 SXM de 80GB de VRAM, n√£o √© us√°vel de verdade.

E eu at√© ficaria triste ou frustrado caso um modelo de 32B fosse realmente, ordens de grandeza melhor que um 14B da vida, mas nos meus testes emp√≠ricos: n√£o √©. Ele faz tanto erro quanto, e n√£o √© t√£o melhor assim que compense pagar t√£o caro por uma infra pr√≥pria topo de linha. Todos eles male male s√≥ conseguem fazer c√≥digos simples, e mesmo assim com erros.

No final, com 10k o server subiu, consumindo absurdos 63GB na inicializa√ß√£o, sem nem come√ßar a processar:

![63GB](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/uetjwlzlgy7bpd4itn1qxczl3mhs?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2020-02-12.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252020-02-12.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001423Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=fb1eec6afc304764383128d1f72c44cc96fc8ba76e5cbd70ab72e74b0d3668fb)


E ...

### Primeiros Testes

Depois de 10 minutos esperando, tive que dar Ctrl+C e come√ßar tudo de novo. Porque eu fui burro. A RunPod √© pra realmente rodar coisas experimentais, n√£o sei se confio pra produ√ß√£o. Ela √© mais um VPS do que uma AWS. E eu estou gambiarrando subindo um pod que espera ter mapeado s√≥ a porta 8888 pra subir um server de Jupyter Notebook (que eu n√£o estou subindo).

![runpod connect](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/h4pzcdxydfiou5iulc7m67zctb9a?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2020-05-17.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252020-05-17.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001424Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=cb465b0763905286f358e50160f9fc35e3899c7c531848007c801ca72ffa2748)

MInha id√©ia √© usar essa porta pra mapear o vLLM. Mas por default ele sobe na porta 8000. Eu esqueci de colocar `--port 8888`:

```
vllm serve Qwen/Qwen3-32b --enable-lora --lora-modules ziglora=./qwen3-zig-lora --max_model_len 10000 --port 8888
```

E l√° vamos n√≥s de novo, esperar mais 10 minutos ... ... e finalmente subiu na porta certa e agora podemos testar. Primeiro, os teste de curl:

![curl hello](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/p8d3ogzwd7lzpu05o7lkm50f8k4h?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2020-23-31.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252020-23-31.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001425Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=ba26f6a8f26a96605b0f1b8e7ad5456353fa2083dc66a9ec6c4d68273fbeb9c8)

E antes que algu√©m comente: n√£o tem problema estar aparecendo a URL do meu pod na foto. Eu vou apagar esse pod antes de publicar este post.

Ali√°s, se subir um modelo remoto, como eu fiz, depois √© s√≥ pegar a URL e colocar na vari√°vel `OPENAI_API_BASE` e qualquer coisa em `OPENAI_API_KEY` e ferramentas como Aider v√£o funcionar igual. A API do vLLM e Ollama s√£o compat√≠veis com da OpenAI.

Como podem ver, a primeira pergunta respondeu parecido com meu teste local com o Qwen3-8B. Nada demais, isso j√° sabia que funcionava. Vamos pedir pra explicar uma funcionalidade nova do Zig 0.14.

Ali√°s, al√©m de lento pra iniciar, tamb√©m fica mais lento pra gerar texto, BEM mais lento. Eu devo ter feito algo errado no treino, mas eu senti a mesma coisa no Lora local ou na H100. Fica notavelmente mais lento. Ele fez mais um "think" longo, mas a resposta desta vez √© mais condizente com o release note (cortei s√≥ o trecho que interessa):

```
In Zig 0.14.0, the `@splat` function has been extended to support arrays, making it easier to initialize fixed-length arrays with a single repeated value. This replaces more verbose or unsafe initialization patterns and is type-safe, with the array's length inferred from the destination type. Here's a concise explanation and code example:

---
...

### **@splat for Arrays**  
`@splat(value: T) [N]T` creates an array of length `N` where every element is `value`. The array's length and element type are determined by the context in which `@splat` is used.  

### **Code Example**  

const Point = struct { x: i32, y: i32 };  

pub fn main() void {  
    // Initialize an array of 5 integers with 42  
    const numbers: [5]i32 = @splat(42);  

    // Initialize an array of 10 booleans with true  
    const flags: [10]bool = @splat(true);  

    // Initialize an array of 7 points with (0, 0)  
    const origin_points: [7]Point = @splat(Point{ .x = 0, .y = 0 });  
}  
....
```

O c√≥digo de exemplo tamb√©m faz um pouco mais de sentido. Comparado com a vers√£o 8B, pelo menos a explica√ß√£o foi melhor. Mas pode ter afetado que eu fiz uma pequena modifica√ß√£o no dataset: eu dupliquei o release notes, pra tentar um pouco mais de √™nfase nas funcionalidades novas. Provavelmente isso tamb√©m fez efeito.

Agora vamos ver se ele consegue fazer algum c√≥digo que funciona. Meu prompt foi este:

```
"Can you give me a Zig 0.14 code example of a simple program that acts like an interactive chat, so the user can input messages. then the message is parsed to see if there are commands such as '/list', which will list the files in the current directory, or '/cat' which will read the file asked, or '/quit' to exit the chat?"
```

E isto aconteceu:

![abort](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/4n3bzm5g0gt46jfmqv55qy0x9tlu?response-content-disposition=inline%3B%20filename%3D%22Screenshot%20From%202025-05-03%2020-36-42.png%22%3B%20filename%2A%3DUTF-8%27%27Screenshot%2520From%25202025-05-03%252020-36-42.png&response-content-type=image%2Fpng&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIA5FTZDKYVLZU6Z457%2F20250527%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250527T001427Z&X-Amz-Expires=300&X-Amz-SignedHeaders=host&X-Amz-Signature=2ed0e0de6585db1a9b034cfdeff050a19a7fbd7750d17dbe66f5314ff5c1a60e)

Lembra como eu tive que ir diminuindo contexto pra conseguir subir? E lembram como eu falo que "thinking" desperdi√ßa muitos tokens? Pois √©. O modelo se auto-crasheou pensando demais. Tentei algumas vezes e ele crasheia. Pra tentar evitar isso, tentei fazer o `curl` mandando `enable_thinking=false` assim:

```bash
‚ùØ curl https://d8zck5aw8eimij-8888.proxy.runpod.net/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "ziglora", "temperature": "0.1", "enable_thinking": "false", "skip_special_tokens": true,
        "messages": [{"role": "user", "content": "..."}]
      }'
```

Temperatura, pra c√≥digo, eu acho que n√£o d√° pra ser muito maior que 0.1. Pra bate-papo, coisas menos exatas, o padr√£o de 0.6 que todo mundo usa √© ok. Mas pra c√≥digo, eu realmente n√£o preciso que ele fique sendo "criativo" (aleat√≥rio, na verdade), eu quero respostas o mais exatas quanto poss√≠vel.

Enfim, mesmo tentando desligar o thinking, alguma coisa est√° errada. Possivelmente o LoRa que eu adicionei est√° fazendo ele "pensar em c√≠rculos" e nunca conseguindo retornar, estourando o contexto. Tentei v√°rias vezes, mas ele est√° sendo incapaz de responder c√≥digo.

### Final: Desisto por enquanto

Sim, eu quebrei a LLM, ao que parece kkkkk üòÖ

Tentei v√°rias vezes e ele realmente n√£o consegue me devolver nenhum c√≥digo. E eu sei que ele est√° tentando, porque se eu mando `stream=true`, eu consigo ver devolvendo tokens, mas em algum momento acaba espa√ßo em algum lugar (com certeza o contexto, que √© pequeno) e crasheia.

A sensa√ß√£o √© que meu LoRa provavelmente "grudou" demais e desbalanceou par√¢metros demais. Agora o modelo come√ßa a escrever o texto e come√ßa a entrar em pensamento circular, sem nunca parar de gerar mais texto. 

Isso com certeza √© efeito do Lora e de algum erro no treinamento. Se eu tiver que chutar, √© porque eu enfiei textos super longos no treinamento, sem indica√ß√µes manuais de aonde acaba cada parte com um `<|im_end|>` da vida. Sem isso, talvez ele ache que o texto inteiro √© uma resposta e ele tem que responder longo assim. Pelo que entendi, no caso desse treinamento de Zig, eu teria que decompor a documenta√ß√£o, o release notes e tudo mais em peda√ßos menores, assim:

```
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
What is Zig?<|im_end|>
<|im_start|>assistant
Zig is a compiled programming language...<|im_end|>
``` 

E fazer isso pra todas as 200 mil caracteres de documenta√ß√£o. Imagina o trabalho pra fazer isso. E evitar enviar textos super longos em contexto de come√ßa e onde acaba cada parte.

Pelo menos, esse √© meu chute. O problema: mesmo se eu gastar tempo formatando assim, da√≠ s√£o mais 6 horas processando, pra s√≥ depois saber se fez alguma diferen√ßa. Provavemente n√£o vai fazer, preciso pensar outra hip√≥teses, e cada vez s√£o 6 a 12 horas entre setup e processamento.

A partir deste ponto √© um trampo que eu n√£o tenho motiva√ß√£o pra fazer. Estou deixando anotado aqui pra ver se isso chega na m√£o de pesquisadores de verdade. Talvez seja algo que um pesquisador de I.A. ache super √≥bvio. Mas este √© o ponto: essa informa√ß√£o n√£o existe f√°cil publicamente em lugar nenhum. Todo exemplo que se acha online s√£o muito simples. S√≥ falam _"nesta linha de python voc√™ carrega o dataset, e nesta pr√≥xima linha come√ßa o treinamento"_ - mas cad√™ as instru√ß√µes de COMO fazer um dataset e COMO determinar se ela √© adequada? N√£o tem.

Se algu√©m tiver dicas de como melhorar o treinamento ou quiser um problema de pesquisa na √°rea, [este √© o GitHub](https://github.com/akitaonrails/qwen3-zig-lora-training) com todos os c√≥digos e material de treinamento que eu fiz. Est√° faltando pesquisadores olharem pra esse tipo de coisa e divulgar melhor o que sabem. Eu sei que algu√©m sabe, mas s√≥ sair fu√ßando no Google n√£o me levou a lugar algum.
